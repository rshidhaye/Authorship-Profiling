{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Profiling\n",
    "\n",
    "### Name: Ruchira Shidhaye\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installing libraries if not present already:\n",
    "\n",
    "#!pip install nltk\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET                                      ## Extracting the data from xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize    \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import *\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import  BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the train_labels and test files into dataframes using pandas:\n",
    "\n",
    "df=pd.read_csv('train_labels.csv')\n",
    "df_test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare an empty list for storing all the content of our xml files belonging to training set one by one:\n",
    "\n",
    "text_train=[]\n",
    "\n",
    "## A loop which iterates through the train_labels dataframe and extracts the data:\n",
    "\n",
    "for i in df['id']:\n",
    "    name=i+'.xml'\n",
    "    name='data/'+name\n",
    "    tree = ET.parse(name)\n",
    "    my_text = [item.text for item in tree.iter()]\n",
    "    text_train.append(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare an empty list for storing all the content of our xml files belonging to test set one by one:\n",
    "\n",
    "text_test=[]\n",
    "\n",
    "## A loop which iterates through the test_labels dataframe and extracts the data:\n",
    "\n",
    "for i in df_test['id']:\n",
    "    name=i+'.xml'\n",
    "    name='data/'+name\n",
    "    tree = ET.parse(name)\n",
    "    my_text = [item.text for item in tree.iter()]\n",
    "    text_test.append(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning to different variables:\n",
    "\n",
    "trainDocs=text_train\n",
    "testDocs=text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the lemmatizer which will convert the words into their dictionary form:\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer()\n",
    "    def __call__(self,doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing steps:\n",
    "\n",
    "- Following are the pre-processing steps which are performed on the text data extracted from xml files in an order:\n",
    "    \n",
    "    1. Removing **https** tags, mentions of another person or account that start with **@**, **#** symbols but the words associated with them are retained.\n",
    "    2. Removing the **emojis** or **emoticons** from the text.\n",
    "    3. Converting all words to lowercase and then tokenizing them using **RegexpTokenizer**.\n",
    "    4. Removing **stopwords** from the tokens.\n",
    "    5. Removing numbers but keeping alphanumeric words.\n",
    "    6. Removing single character words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining the data in text into a string:\n",
    "\n",
    "trainDocs = [\" \".join(x) for x in text_train]\n",
    "testDocs = [\" \".join(x) for x in text_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to remove emojis:\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of stopwords belonging to english language:\n",
    "\n",
    "stoplist = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing of training texts:\n",
    "\n",
    "## A tokenizer that splits a string using a regular expression, which matches the tokens as gaps is False:\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\", gaps=False)  \n",
    "\n",
    "## Iterating through training set:\n",
    "\n",
    "for i in range(len(trainDocs)):\n",
    "    trainDocs[i]=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',trainDocs[i])   ## Removing the https tags\n",
    "    trainDocs[i]=re.sub('@[^\\s]+','',trainDocs[i])                             ## Removing the mentions of people\n",
    "    trainDocs[i]=re.sub(r'#([^\\s]+)', r'\\1',trainDocs[i])                      ## Removing the # symbol but keeping the word attached with it\n",
    "    trainDocs[i]=deEmojify(trainDocs[i])                                       ## Removing the emojis\n",
    "    trainDocs[i] = trainDocs[i].lower()                                        ## Converting to lowercase\n",
    "    trainDocs[i] = tokenizer.tokenize(trainDocs[i])                            ## Split into words\n",
    "    nst=[]                                                                     ## A list to hold words not in stopwords list\n",
    "    for j in trainDocs[i]:\n",
    "        if j not in stoplist:\n",
    "            nst.append(j)\n",
    "    trainDocs[i]=nst                                                           ## Finally appending clean tokens\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove numbers, but not words that contain numbers:\n",
    "\n",
    "trainDocs = [[token for token in doc if not token.isnumeric()] for doc in trainDocs]\n",
    "\n",
    "## Remove words that are only one character:\n",
    "\n",
    "trainDocs = [[token for token in doc if len(token) > 1] for doc in trainDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing of testing texts:\n",
    "\n",
    "## A tokenizer that splits a string using a regular expression, which matches the tokens as gaps is False:\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\", gaps=False)  \n",
    "\n",
    "## Iterating through testing set:\n",
    "\n",
    "for i in range(len(testDocs)):\n",
    "    testDocs[i]=re.sub('(www|http:|https:)+[^\\s]+[\\w]','',testDocs[i])        ## Removing the https tags\n",
    "    testDocs[i]=re.sub('@.*?\\s+','',testDocs[i])                              ## Removing the mentions of people\n",
    "    testDocs[i]=re.sub(r'#([^\\s]+)', r'\\1',testDocs[i])                       ## Removing the # symbol but keeping the word attached with it\n",
    "    testDocs[i]=deEmojify(testDocs[i])                                        ## Removing the emojis\n",
    "    testDocs[i] = testDocs[i].lower()                                         ## Converting to lowercase\n",
    "    testDocs[i] = tokenizer.tokenize(testDocs[i])                             ## Split into words.\n",
    "    nst=[]                                                                    ## A list to hold words not in stopwords list\n",
    "    for j in testDocs[i]:\n",
    "        if j not in stoplist:\n",
    "            nst.append(j)\n",
    "    testDocs[i]=nst                                                           ## Finally appending clean tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove numbers, but not words that contain numbers:\n",
    "\n",
    "testDocs = [[token for token in doc if not token.isnumeric()] for doc in testDocs]\n",
    "\n",
    "## Remove words that are only one character:\n",
    "\n",
    "testDocs = [[token for token in doc if len(token) > 1] for doc in testDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming text to feature vectors that can be used as input to estimator using TFIDF Vectorizer:\n",
    "\n",
    "## Setting analyzer to word for outputting words and phrases of ngram_range(unigrams, bigrams and trigrams)\n",
    "## Setting min_df=3 indicating to not consider words appearing in less than 3 documents and max_df to 3050 meaning not to\n",
    "## consider words appearing in more than 3050 documents:\n",
    "\n",
    "word_vector=TfidfVectorizer(analyzer='word',input='content',\n",
    "                           min_df=3,ngram_range=(1,3),max_df=3050,           \n",
    "                          tokenizer=LemmaTokenizer()\n",
    "                           )\n",
    "\n",
    "## Setting analyzer to char for outputting character ngrams:\n",
    "\n",
    "char_vector=TfidfVectorizer(analyzer='char',input='content',\n",
    "                           min_df=0,ngram_range=(2,3),\n",
    "                          tokenizer=LemmaTokenizer()\n",
    "                           )                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using FeatureUnion to combine word and character features:\n",
    "\n",
    "vectorizer = FeatureUnion([(\"chars\",char_vector),(\"words\",word_vector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the data into a format which is suitable for TFIDF Vectorizer for training set:\n",
    "\n",
    "inputc = [\" \".join(x) for x in trainDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the TFIDF Vectorizer on the training text:\n",
    "\n",
    "x_train=vectorizer.fit_transform(inputc)\n",
    "y_train=np.asarray(df['gender'])                                             ## Training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161412"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the data into a format which is suitable for TFIDF Vectorizer for test set:\n",
    "\n",
    "input_test = [\" \".join(x) for x in testDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the TFIDF Vectorizer on the testing text:\n",
    "\n",
    "x_test=vectorizer.transform(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual testing labels:\n",
    "\n",
    "test_actual=pd.read_csv('test_labels.csv')\n",
    "y_test=test_actual.gender.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier\n",
      "[[212  40]\n",
      " [ 38 210]]\n",
      "Accuracy: 0.844\n",
      "Macro Precision: 0.844\n",
      "Macro Recall: 0.8440220174091142\n",
      "Macro F1 score:0.8439975039600633\n",
      "MCC:0.6880220170568243\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(94)                                                  ## Setting the random seed for reproducibility of results\n",
    "\n",
    "from warnings import simplefilter\n",
    "## ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "models = [  \n",
    "    \n",
    "    ## Define the PassiveAggressiveClassifier model to be used with regularization parameter set to 5.2:\n",
    "\n",
    "    BaggingClassifier(PassiveAggressiveClassifier(C=5.2))               \n",
    "\n",
    "]\n",
    "\n",
    "for clf in models:                                                      ## Run the loop to use one model at each iteration in case of multiple models\n",
    "    model_name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)                                           ## Fit the model on the train set\n",
    "    print(model_name)\n",
    "    \n",
    "                                                                        ## Prediction on test set\n",
    "    y_predict=clf.predict(x_test)   \n",
    "    print(confusion_matrix(y_test,y_predict))\n",
    "    recall=recall_score(y_test,y_predict,average='macro')\n",
    "    precision=precision_score(y_test,y_predict,average='macro')\n",
    "    f1score=f1_score(y_test,y_predict,average='macro')\n",
    "    accuracy=accuracy_score(y_test,y_predict)                           ## Accuracy score between true and predicted labels\n",
    "    matthews = matthews_corrcoef(y_test,y_predict) \n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Macro Precision: '+ str(precision))\n",
    "    print('Macro Recall: '+ str(recall))\n",
    "    print('Macro F1 score:'+ str(f1score))\n",
    "    print('MCC:'+ str(matthews))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the predicted labels to gender column of test dataframe:\n",
    "\n",
    "df_test['gender']=y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the language column from the test dataframe:\n",
    "\n",
    "del df_test['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the predictions to a csv file:\n",
    "\n",
    "df_test.to_csv('pred_labels.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<li>https://stackoverflow.com/questions/41089578/finding-xml-text-content-from-tag-name-in-python\n",
    "    <li>https://stackoverflow.com/questions/43797500/python-replace-unicode-emojis-with-ascii-characters/43813727#43813727\n",
    "        <li>https://www.slideshare.net/PyData/authorship-attribution-forensic-linguistics-with-python-scikit-learn-pandas-kostas-perifanos\n",
    "            <li>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
